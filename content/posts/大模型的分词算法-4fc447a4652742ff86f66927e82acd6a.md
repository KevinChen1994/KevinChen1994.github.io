---
title: "大模型的分词算法"
date: "2023-12-06T02:33:00.000Z"
lastmod: "2023-12-06T06:28:00.000Z"
draft: false
featuredImage: "https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-8\
  2ce-4f96-ae1a-879bd6c9f3a6/cbbc7f52-197c-4106-9b0f-71c5c7070468/tok_hf.png?X-\
  Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Cr\
  edential=ASIAZI2LB4664276JFU2%2F20250216%2Fus-west-2%2Fs3%2Faws4_request&X-Am\
  z-Date=20250216T132400Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX\
  2VjEDQaCXVzLXdlc3QtMiJGMEQCHxTGWQZo4fpY%2BJpjT44EhA904euN67xr%2FoiWP7knNvsCIQ\
  CG%2B2V7qc2uhR1ut89Y38u7AsKVxQUvES5zUFd%2BUzX4ECr%2FAwhdEAAaDDYzNzQyMzE4MzgwN\
  SIMDryZkNwPUIeLR%2Fi3KtwDsaAbr8f21z%2FnzLhAbQWkI2YteTRF%2FS3vi06QVejOhql2OFXR\
  0PzWzZDItP0X3BO4iQqC3DXEJrGUgFNBGjd3Ff3xDxh97rDgH7BMLrwNTvFkck0ZLD23Y3F6%2BMx\
  2iniNChBEyPLTeTgk42e8Gw2rxE6W5Uk630rTBKpBDcnRRxnhNr%2FqjUUHt4PIdzvUPfW7xJUUly\
  Qco4Ubu9WVxozyx%2B0er2LJTaoo1I%2Fxvl17ClkF7QvVHCOgrZY4dYW48LdRrRpI2Ek430g1coz\
  mCGh4to5K0IcOlQYgs8v6kqegRjWjQeeArRPQD1iELlRWouYb2hUXgKSfIx7r7Blon9GzteqS57ay\
  cZshpGiCAb6q77GPr1SCyL6hIW37NnA%2BirtPAFlo2NR463RhAbeAHmwIVTIf2eJq3lS3anptkZn\
  q1yf3hi4Yw54kLgnbMTCTvii3EXjEHloTeEtYS76WCZI2cOMxmzF9HTcrkeaLyilfkmLr%2FwEmyS\
  SZT67%2BI5%2Fy1loZLG%2F%2BBPMXDBRUsd6l6jofnzeBqH0zPNy72bO%2Bx4Pj2vgtmCSlRmdw1\
  Y%2BlP2AbfckVgBT7vXsuADhuOzioNx5BnTFCdsEaP%2F5mO%2BBL4HGnq9QGfyro7%2F892mc15x\
  UwkaHHvQY6pgHCfhBXxYg2KQWQFgSDmm7HremJ%2F3BxRLR1YYptdzyQZDvPVQgQfcU1SCrMlW9rV\
  KP3itChAzsr%2FSLUkmPpoK%2BaToG68u992wjwxRt5DXSkVqzIm%2Fn40bWbOUbZyH7F%2F8NjSA\
  ldpEOlqb4uRxlBgYY8u4v6%2F%2BYmltxohcV4wrVH%2FmMMT0YKi8OyxKiJ1yl6SGgmKv4VP%2F7\
  C5BdAWscBpCXWzir4ievU&X-Amz-Signature=3ba1537685bcf7c3b79e2abf4b3b006c4619a83\
  0d051fe8451f4fcc1843b95e8&X-Amz-SignedHeaders=host&x-id=GetObject"
series:
  - "Tech"
authors:
  - "陈猛"
tags:
  - "tokenizer"
categories:
  - "LLM"
summary: "大模型如何读懂自然语言，需要我们将文本转成机器能读懂的数字，这篇文章介绍了一些从文本到数字的算法。"
NOTION_METADATA:
  object: "page"
  id: "4fc447a4-6527-42ff-86f6-6927e82acd6a"
  created_time: "2023-12-06T02:33:00.000Z"
  last_edited_time: "2023-12-06T06:28:00.000Z"
  created_by:
    object: "user"
    id: "cc08a802-cdc1-4040-b261-957206a41bd5"
  last_edited_by:
    object: "user"
    id: "cc08a802-cdc1-4040-b261-957206a41bd5"
  cover:
    type: "file"
    file:
      url: "https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-a\
        e1a-879bd6c9f3a6/cbbc7f52-197c-4106-9b0f-71c5c7070468/tok_hf.png?X-Amz-\
        Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-\
        Credential=ASIAZI2LB466UXO7BHUR%2F20250216%2Fus-west-2%2Fs3%2Faws4_requ\
        est&X-Amz-Date=20250216T132257Z&X-Amz-Expires=3600&X-Amz-Security-Token\
        =IQoJb3JpZ2luX2VjEDQaCXVzLXdlc3QtMiJGMEQCIFE952Sg%2FCxVe5VEEJuDdKQdTkvu\
        9yVqEFyvw2dhfS1cAiACBGLgTBPEFIGg4v41qnM%2BTB4U%2FUnh7EoNVQXyVTTqTyr%2FA\
        whdEAAaDDYzNzQyMzE4MzgwNSIMTldsOy9E%2Br6YpHAiKtwDWsQI9vduoSXOPvPMfs219I\
        1Qly%2Bgu0AJn%2BxdLR6h27UNWBfQmTyq9b4uaKhBiIdf3fmgWsCBtgBlHnCzpSNdmdPSu\
        vTvwyVHf1tD0jrpa3IrlWjsAn7K1tDJtBnCjHIwUMC78pNhFMP1Vk4qfNoP%2FfB%2Fn8zR\
        MbtnI7hSUU9W9O7Lmvo0p3BkyhlHixeAPRy2AoGtzrGjWCnOf8OrvV5kioiMKXgrHNeXvGp\
        RExAlLYNwyilpNs2bWsB8XTJTSzmZs01Hkpd6QaIUtMNwco2vUz%2BFsTkuMBDi%2BBy8gJ\
        HhM%2FKUtbmOy036rhV%2F2BqAdIFlgl5pL6njF52F9CPQkt3YmtpYVtPvVVymusvLgI242\
        aU5XMmqX2yMldNDapwupDm2SHwHC5R72wg%2BCH7doplyow6CZ%2B1Ai9yt594b9CYhoILM\
        4OK1O1r1D34Y8OyV0wk7NbJYC5exrYhRuoZqGVX%2Fb7pvTUSVlNbBOW22n6dlFgHF3Y26a\
        2zCF91bZINGoLvm%2FfCtXeMbIbTzYI1Z2PnHe7IY%2FhxysEX7mXh4LSMcgq2C58OvQDTU\
        pgTG2oeAvcXxaspN30h7yJKCeq2alM4rdx2w6I3ieQ8LFYk%2BT%2FLY12gVvPSNDvFVVfT\
        Af8owgKTHvQY6pgHBKtj04iWW16aF1iE9%2BSC7DXmjkqSyL5eGCxbAS%2F07uHnC7Ajlvh\
        TZf3icliJ2I2%2BNFrDnhL0fy0mu9LKQ5cMSqUq3LHJZgYiH88HWqcb0VPFloJX1xPw4NTR\
        lpNK9iVGLqUvEFoGvBfhuAyK1buA10zx4tJ7VWuSU84MtxuofYWLFVCtLXh0tgXJfZUkMzX\
        lbDslzCBrCZPh1YuEgRNu4UQf7gKy%2B&X-Amz-Signature=173df7412540e07a313079\
        9f7856fa0c3b22b565c6a500727965bd715ad5ae0c&X-Amz-SignedHeaders=host&x-i\
        d=GetObject"
      expiry_time: "2025-02-16T14:22:57.529Z"
  icon: null
  parent:
    type: "database_id"
    database_id: "8d6a6f9d-5a2c-433b-a560-b744eab9db1a"
  archived: false
  in_trash: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select:
        - id: "f6345faf-6e79-413e-847a-3fb764a61e06"
          name: "Tech"
          color: "green"
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    Created time:
      id: "UBQ%7B"
      type: "created_time"
      created_time: "2023-12-06T02:33:00.000Z"
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "cc08a802-cdc1-4040-b261-957206a41bd5"
          name: "陈猛"
          avatar_url: "https://s3-us-west-2.amazonaws.com/public.notion-static.com/775523\
            b7-57cf-4c98-8ad8-8777d898666f/notion-avatar-1678713535269.png"
          type: "person"
          person:
            email: "346521888@qq.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "2abdee76-a9ad-41fb-a7c2-4286dc254bdb"
          name: "tokenizer"
          color: "purple"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "e417d9a1-8454-498a-b9de-502d57e26681"
          name: "LLM"
          color: "gray"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text:
        - type: "text"
          text:
            content: "大模型如何读懂自然语言，需要我们将文本转成机器能读懂的数字，这篇文章介绍了一些从文本到数字的算法。"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "大模型如何读懂自然语言，需要我们将文本转成机器能读懂的数字，这篇文章介绍了一些从文本到数字的算法。"
          href: null
    Date:
      id: "zYLY"
      type: "date"
      date: null
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "大模型的分词算法"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "大模型的分词算法"
          href: null
  url: "https://www.notion.so/4fc447a4652742ff86f66927e82acd6a"
  public_url: "https://kevinchen1994.notion.site/4fc447a4652742ff86f66927e82acd6a"
UPDATE_TIME: "2025-02-16T13:24:03.474Z"
EXPIRY_TIME: "2025-02-16T14:24:00.041Z"

---
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">


看了一些介绍tokenizer算法的文章，感觉大家介绍的时候并不是很系统，甚至有的文章还有一些错误，所以我在这篇文章做了一个汇总，没有个人原创的内容。


| **分词方法**      | **典型模型**                                                 |
| ------------- | -------------------------------------------------------- |
| BPE           | GPT,GPT-J, GPT-Neo, RoBERTa, BART,  ChatGLM-6B, Baichuan |
| BBPE          |  GPT-2, LLaMA                                            |
| UniLM         | mBART, XLNet                                             |
| WordPiece     | BERT, DistilBERT，MobileBERT                              |
| SentencePiece | ALBERT, T5, Flan T5, XLM-RoBERTa                         |


## BPE


> 💡 输入：训练语料； 词表大小V  
>   
> 1.准备足够大的训练语料，确定期望的subword词表大小；  
> 2.准备基础词表：比如英文中26个字母加上各种符号；  
> 3.基于基础词表将语料中的单词拆分为字符序列并在末尾添加后缀“ </ w>”；本阶段的subword的粒度是字符。例如单词“ low”的频率为5，那么我们将其改写为“ l o w </ w>”：5；  
> 4.统计每一个连续字节对的出现频率，选择最高频的字符对合并成新的subword；  
> 5.重复第4步直到达到第1步设定的subword词表大小或下一个最高频的字节对出现频率为1；


代码参考这篇文章，知乎几乎所有的代码都是来自于这里


[https://leimao.github.io/blog/Byte-Pair-Encoding/](https://leimao.github.io/blog/Byte-Pair-Encoding/)


```python
'''
https://leimao.github.io/blog/Byte-Pair-Encoding/
'''

import re, collections

def get_vocab(filename):
    vocab = collections.defaultdict(int)
    with open(filename, 'r', encoding='utf-8') as fhand:
        for line in fhand:
            words = line.strip().split()
            for word in words:
                vocab[' '.join(list(word)) + ' </w>'] += 1

    return vocab

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

def get_tokens_from_vocab(vocab):
    tokens_frequencies = collections.defaultdict(int)
    vocab_tokenization = {}
    for word, freq in vocab.items():
        word_tokens = word.split()
        for token in word_tokens:
            tokens_frequencies[token] += freq
        vocab_tokenization[''.join(word_tokens)] = word_tokens
    return tokens_frequencies, vocab_tokenization

def measure_token_length(token):
    if token[-4:] == '</w>':
        return len(token[:-4]) + 1
    else:
        return len(token)

def tokenize_word(string, sorted_tokens, unknown_token='</u>'):
    
    if string == '':
        return []
    if sorted_tokens == []:
        return [unknown_token]

    string_tokens = []
    for i in range(len(sorted_tokens)):
        token = sorted_tokens[i]
        token_reg = re.escape(token.replace('.', '[.]'))

        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]
        if len(matched_positions) == 0:
            continue
        substring_end_positions = [matched_position[0] for matched_position in matched_positions]

        substring_start_position = 0
        for substring_end_position in substring_end_positions:
            substring = string[substring_start_position:substring_end_position]
            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)
            string_tokens += [token]
            substring_start_position = substring_end_position + len(token)
        remaining_substring = string[substring_start_position:]
        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)
        break
    return string_tokens

# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}

vocab = get_vocab('../data/pg16457.txt')

print('==========')
print('Tokens Before BPE')
tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)
print('All tokens: {}'.format(tokens_frequencies.keys()))
print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))
print('==========')

num_merges = 10000
for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print('Iter: {}'.format(i))
    print('Best pair: {}'.format(best))
    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)
    print('All tokens: {}'.format(tokens_frequencies.keys()))
    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))
    print('==========')

# Let's check how tokenization will be for a known word
word_given_known = 'mountains</w>'
word_given_unknown = 'Ilikeeatingapples!</w>'

sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)
sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]

print(sorted_tokens)

word_given = word_given_known 

print('Tokenizing word: {}...'.format(word_given))
if word_given in vocab_tokenization:
    print('Tokenization of the known word:')
    print(vocab_tokenization[word_given])
    print('Tokenization treating the known word as unknown:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))
else:
    print('Tokenizating of the unknown word:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))

word_given = word_given_unknown 

print('Tokenizing word: {}...'.format(word_given))
if word_given in vocab_tokenization:
    print('Tokenization of the known word:')
    print(vocab_tokenization[word_given])
    print('Tokenization treating the known word as unknown:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))
else:
    print('Tokenizating of the unknown word:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))

'''
Tokenizing word: mountains</w>...
Tokenization of the known word:
['mountains</w>']
Tokenization treating the known word as unknown:
['mountains</w>']
Tokenizing word: Ilikeeatingapples!</w>...
Tokenizating of the unknown word:
['I', 'like', 'ea', 'ting', 'app', 'l', 'es!</w>']
'''
```


## BBPE


对于英文来说，使用BPE能够在词表可控的前提下解决OOV的问题，但是对于中文、日文等语言时，稀有文字可能会不必要的占用词表大小。所以BBPE方法是将一本文本的UTF-8编码中的一个字节256位不同编码作为词表的初始化Subword。


相比ASCII只能覆盖英文中字符，**UTF-8编码创建的本身就是为了通用的将世界上不同的语言字符尽可能全部用一套编码进行编号**，同时相比UTF-32对于每个字符都采用4位字节（byte）过于冗长。改进的UTF-8编码是一个变长的编码，有1～4个范围的字节(bytes)长度。对于不同语言中字符采用不同长度的字节编码，例如英文字符基本都是1个字节（byte），中文汉字通常需要2～3个字节。


核心思想是用byte来构建最基础的词表而不是字符。首先将文本按照UTF-8进行编码，每个字符在UTF-8的表示中占据1-4个byte。 在byte序列上再使用BPE算法，进行byte level的相邻合并。编码形式如下。


![](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/44df21ab-260b-49d9-b8fb-021817534d3c/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466QFEXIDZP%2F20250216%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250216T132400Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDQaCXVzLXdlc3QtMiJHMEUCIQD2oABkb1zbaFsA7E%2Fx%2BqnmXZoXD2Tz2nIk0x5SvWO4JgIgTPi2TU2IqhBBEKtUind9AJxMK4TxETOhqT10Rl0sEi0q%2FwMIXRAAGgw2Mzc0MjMxODM4MDUiDDzhjNTMLxE9%2B%2FiFiircA4XFsHgl%2BAHwxIXb4vueqUzhzeZSreJPTBCzJ5hcyLdwBw%2B%2BR4nXLQkPZ5r%2Fw7jbnHAKhVpRNAxxTg8T8nwX86ag2Rnx%2FsOL6iAKJGGNlcM1cB2D00So%2FKsoz2nC96MSPlt7d%2F53laIXhdBa1muH36m%2F4w4oMsIky9eQG69L%2FUByrVpd55%2F8FVobF47104muKJeXXIS4ZOS90NHMtRXJ4ROJndjjXaTvvF8LJADWWfWy%2B1O9hfefNjtxxTpnQzPD4l1CvTnniWst3owXkZBoJ4og5%2B6vt%2F7a2iE8vUFayq57N9XWeZ8HqaJj7fDwpsh5ji0%2BfDSWQuab2SqU2N%2F3GveMpKR%2Fs8AFNVV%2FBZY%2B3OjlliE6rXWQ3K4DAaQzmyrdhLwhQ3m%2FV%2BFyOK6NI27S0shiHNiuRT3jOJJHw2K%2BCSGgahc8m2Sq2pkF4QVKcikY30gD88wXedUKSJi3KrAYtc6TOk0untoJQqx%2FXGeCbboD6PhFVu8EGR8Iz6dZMAj6qOQujKLyEdu6jmTuq2Obdntf7F53nx5i0QGqYBT6N4zsGbQs78%2Fr6uLmIr5Bq8O08r8rNUsXaIv7aBKpgWxwFX%2FlnGSvV%2F6tSf9ImKwtBF7S33rfOs1ieJT11fJZMNmbx70GOqUBZP5b6m9eBHCAsblhrtcRUSvdtZzeM3DMFdjOWVzAb7bQ5VRqLmVw2n%2FXjn8fGXYuuuok%2BaSTcFWUrNiHdK2WsK7wJ%2BwVmBH1StCfN4x%2BbXNVg%2BIfMv7Pcas83Uu24mJTGTFpBzgqM33LE53S0mOJby90hmrhc6rDt0luGOZQ2GnzRZhsdv1gIsw1YE1dICYj1GY7tLqqygNOvkrEhl56KNczlG1Y&X-Amz-Signature=37031a75a8d9a830c39fa48f88a9fef250a753f577e6796effca3a17b7cccdbc&X-Amz-SignedHeaders=host&x-id=GetObject)


在解码阶段，一个byte序列可能解码后不是一个合法的字符序列，这里需要动态规划进行解码，使其能解码出尽可能多的合法字符。所以BBPE虽然解决了中文、日文词表过大的问题，但是解码上还是有一些问题。


## **WordPiece**


> 💡 输入：训练语料； 词表大小V  
>   
> 1.准备足够大的训练语料，确定期望的subword词表大小；  
> 2.准备基础词表：比如英文中26个字母加上各种符号；  
> 3.基于基础词表将语料中的单词拆分为最小单元;  
> 4.基于第3步数据训练语言模型，可以是最简单的unigram语言模型，通过极大似然进行估计即可;  
> 5.从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元;  
> 6.重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值;


WordPiece算法跟BPE算法流程基本一致，BPE是基于最高词频来生成Subword，而WordPiece是基于概率来生成Subword。具体来说，有一个词为z，z由x和y组成，对比P(Z)的概率和P(X)+P(Y)的概率，如果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>Z</mi><mo stretchy="false">)</mo><mo>&gt;</mo><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>+</mo><mi>P</mi><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(Z)&gt;P(X)+P(Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，则合并x和y，因为他们具有最的互信息，也就是在语言模型上具有较强的关联性。


我们假设句子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S=(t_1,t_2,...,t_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>代表一个子词，且假设各个子词之间是相互独立存在的，那么句子的语言模型似然值等价于所有子词概率的乘积。


<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">logP(S) = \sum_{i}^nlogP(t_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>


假设把相邻位置的x和y两个子词进行合并，合并后产生的新的字词为z，此时句子S的似然值变化可以表示为：


<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>z</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>x</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>y</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>z</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>x</mi></msub><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>y</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">logP(t_z)-(logP(t_x)+logP(t_y))=log(\frac{P(t_z)}{P(t_x)P(t_y)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3991em;vertical-align:-0.9721em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span>


从上面的公式可以发现，似然值的变化就是两个子词的互信息。简而言之，WordPiece每次选择合并的两个子词，他们具有最大的互信息值，也就是相关字词在语言模型上具有较强的关联性，他们经常在语料中以相邻的方式同时出现。


## UniLM


> 💡 输入：训练语料；词表大小V； 保留阈值X；  
>   
> 1.准备基础词表：初始化一个很大的词表，比如所有字符+高频Ngram，也可以通过BPE算法初始化；  
> 2.针对当前词表，用语言模型（unigram lm)估计每个子词在语料上的概率；  
> 3.计算删除每个Subword后对总loss的影响及Score得分，作为该Subword排序的Score得分；  
> 4.将子词按照Score大小进行排序，保留前X%的Subword；注意，建议单字符的Subword不能被丢弃，以免OOV；  
> 5.重复步骤2到4，直到词表大小减少到设定值；


UniLM可以看成是WordPiece算法在执行过程中进行反向操作。相比WordPiece借助语言模型选择合并计算概率最大的相邻字符对加入词表中作为新的Subword，UniLM是开始时先构建足够大的词表，之后每一步选择一定比例的计算概率低的Subword从词表中删除。因此过程中比较显著差别是WordPiece算法的词表在生成过程中是从小到大的变化，而UniLM的词表则是从大到小的变化，整个过程根据评估不断删除排序靠后的Subword直到词表大小减少到设定值。


## **SentencePiece**


SentencePiece是Google推出的分词工具，这个包主要是为了多语言模型设计的，使用unicode编码，解决了多语言编码方式不同的问题。

- 内置BPE，Unigram，char和word的分词方法
- 无需预分词，以unicode方式直接编码整个句子，空格会被特殊编码为▁
- 相比传统实现进行优化，分词速度速度更快

中文llama模型使用的就是SentencePiece进行扩充中文词表。具体流程为：先使用大量的中文预料进行训练中文tokenizer模型，然后将中文模型与llama tokenizer模型进行合并，合并代码：


[https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_tokenizer/merge_tokenizers.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_tokenizer/merge_tokenizers.py)


## 参考文章


[https://mp.weixin.qq.com/s/Sgz74bSs0saCFhw1GOaMnw](https://mp.weixin.qq.com/s/Sgz74bSs0saCFhw1GOaMnw)


[https://zhuanlan.zhihu.com/p/86965595](https://zhuanlan.zhihu.com/p/86965595)


[https://zhuanlan.zhihu.com/p/649030161](https://zhuanlan.zhihu.com/p/649030161)


[https://zhuanlan.zhihu.com/p/651430181](https://zhuanlan.zhihu.com/p/651430181)


[https://huggingface.co/docs/transformers/tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary)

