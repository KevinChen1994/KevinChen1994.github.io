---
title: "Mixtral 8x7B MoE模型笔记"
date: "2024-01-05T01:18:00.000Z"
lastmod: "2024-06-12T09:08:00.000Z"
draft: false
series: []
authors:
  - "陈猛"
tags:
  - "MoE"
categories:
  - "LLM"
summary: "随着 Mixtral 8x7B 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的
  Transformer 模型在开源人工智能社区引起了广泛关注。"
NOTION_METADATA:
  object: "page"
  id: "d99a1952-b7fd-48cd-8182-9c4965664b75"
  created_time: "2024-01-05T01:18:00.000Z"
  last_edited_time: "2024-06-12T09:08:00.000Z"
  created_by:
    object: "user"
    id: "cc08a802-cdc1-4040-b261-957206a41bd5"
  last_edited_by:
    object: "user"
    id: "cc08a802-cdc1-4040-b261-957206a41bd5"
  cover: null
  icon: null
  parent:
    type: "database_id"
    database_id: "8d6a6f9d-5a2c-433b-a560-b744eab9db1a"
  archived: false
  in_trash: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    Created time:
      id: "UBQ%7B"
      type: "created_time"
      created_time: "2024-01-05T01:18:00.000Z"
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "cc08a802-cdc1-4040-b261-957206a41bd5"
          name: "陈猛"
          avatar_url: "https://s3-us-west-2.amazonaws.com/public.notion-static.com/775523\
            b7-57cf-4c98-8ad8-8777d898666f/notion-avatar-1678713535269.png"
          type: "person"
          person:
            email: "346521888@qq.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "7fe9c3d7-f18a-495a-a55a-3e8a9ab43af2"
          name: "MoE"
          color: "default"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "e417d9a1-8454-498a-b9de-502d57e26681"
          name: "LLM"
          color: "gray"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text:
        - type: "text"
          text:
            content: "随着 Mixtral 8x7B 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的
              Transformer 模型在开源人工智能社区引起了广泛关注。"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "随着 Mixtral 8x7B 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的
            Transformer 模型在开源人工智能社区引起了广泛关注。"
          href: null
    Date:
      id: "zYLY"
      type: "date"
      date: null
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "Mixtral 8x7B MoE模型笔记"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "Mixtral 8x7B MoE模型笔记"
          href: null
  url: "https://www.notion.so/Mixtral-8x7B-MoE-d99a1952b7fd48cd81829c4965664b75"
  public_url: "https://kevinchen1994.notion.site/Mixtral-8x7B-MoE-d99a1952b7fd48c\
    d81829c4965664b75"
UPDATE_TIME: "2025-02-06T07:22:53.049Z"
EXPIRY_TIME: "2025-02-06T08:22:47.044Z"

---
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">


## 前言


大模型时代模型的参数量越来越大，GPT3的模型参数达到了175B，各大厂商也在不断突破模型参数量的天花板。模型参数量增大显而易见的好处就是模型的能力越来越强，并且模型的参数量达到一定的规模后，模型就会出现涌现能力（Emergent Abilities），而带来的坏处也是很明显的，那就是训练和推理的硬件成本不断增加。


2023年12月，Mistral AI在开源社区扔了一条磁力链接，引爆了社交网络。Mistral AI基于混合专家模型Mixture of Experts（MoE），证明通过8个7B的模型就能超越LLaMA 2 70B模型的效果，甚至部分超越了GPT 3.5的水平。之前就有人分析过GPT4就是使用8个专家模型组成的专家系统，这给我们带来了很多启发，是否未来大模型未来会朝着这个方向发展呢？


![](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/74b12913-f31e-4dca-acca-ccbd24cf586e/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466VPS2P5VE%2F20250206%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250206T072247Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjED8aCXVzLXdlc3QtMiJGMEQCIEnVbzYnO%2Fos0uJKmM4ALApdLEDreN9al87BzNk%2BMQuJAiBvTKEKYqk3LCkNiTYjeaQQZThIIzidiqYe5gtIO2YyoSr%2FAwhYEAAaDDYzNzQyMzE4MzgwNSIMnHUqvuO%2FFcFOdvkiKtwDbq0SSihwCXMvkZX4oDal5XpwLLoK%2Fg1DV9tjOYp0zwZFW7UQzaOVZ6hxresZyEenda9rHY5OcUF6E8Wsm5sCPuyrxrecMC4VQAE1mNiV33Sm8cqsuqcyJSMVgATWOjVYQQ7mELCr4A0GReRXn8V0pU7V7CmPNfQBaWVj5nkWJz0pJRvBzjVAlqJ5msxyHCGT4TOdzqISZ3tBEkQFlikmTWpQIoXf70C8U9b1oRRM9D0nd1egiOzon9aOoSMyPPMsCxPf%2FUioHAHbq%2B5wupwDNtnXDTcThGzPgDYxXcLDvRuW6jV4OSNo4oRNdL82cGHN91vFnoc2MSsMsRgAPvqyn%2BOc1D8cOLAl2ry351Iggp0IyhEkJ7qOSoHPkTM%2F%2FNPmicpv6mXIARUWaX%2Fonucs8PmjXSBKhUznmduU1ZMQbrqR0YnedoVwB%2F%2F1aALBsgFvGSOJujY%2BlLCzmdNR0BzUbqB4Hk8rfklFOf%2BfdylVSS3OhnOH2bT7qgvocbmidjZ24IoypsUGxMiLMBEHUH25yKcezuSt1JIQ04VHos7nash6QNoXofnNeF9CYJUoteAKFk7L6fEpe2ZT%2Fqh%2Fvbosh1J9aFugnkQE%2BlCSGUcw5%2ByP1X0vELdkCyM8dn0wnLaRvQY6pgHNwiQX3mR1hPH9nC5hqRa%2FcJ9xAuhGv5R%2FdP2Fcsstsy0NdJs1ixILPNNnOmKMVoopiI5DQ9gBO0DwGLiEsvU0m%2B%2BSE080HeSO7VuIXSoimoLNmkOWB%2BTU4K20B4qdwIk247BMZhmkbJCfZTDGfbEqMljhbAitfy7r6vn75zpoBHnH2lVcsUneW2uyCf0W2%2F%2FsWsdnfmArX8ZbhxChvtgj8baVWYM6&X-Amz-Signature=7f902245459bea34719db7c3873df282d528ea8d1b227abd983cc5991718d091&X-Amz-SignedHeaders=host&x-id=GetObject)


## Mixtral 8x7B模型架构


Mixtral 8x7B与LLaMA模型的区别就是在attention计算中将MLP Layer替换成了一个门控层和8个专家模型，通过门控层会给出每个专家层的权重，每个token会选择top2的专家进行计算， 这使得模型训练和推理的速度相比于LLaMA 2 70B会显著提高。


不过这里有一个误区，那就是模型虽然叫8x7B，但是模型的参数并不是56B，因为在每个层中只有专家层是独立存在的，其他部分如attention是权重共享的，所以模型的参数量在47B左右。


![](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/0775893f-4189-40b0-a47f-cf4a45d07bff/moe.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466VPS2P5VE%2F20250206%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250206T072247Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjED8aCXVzLXdlc3QtMiJGMEQCIEnVbzYnO%2Fos0uJKmM4ALApdLEDreN9al87BzNk%2BMQuJAiBvTKEKYqk3LCkNiTYjeaQQZThIIzidiqYe5gtIO2YyoSr%2FAwhYEAAaDDYzNzQyMzE4MzgwNSIMnHUqvuO%2FFcFOdvkiKtwDbq0SSihwCXMvkZX4oDal5XpwLLoK%2Fg1DV9tjOYp0zwZFW7UQzaOVZ6hxresZyEenda9rHY5OcUF6E8Wsm5sCPuyrxrecMC4VQAE1mNiV33Sm8cqsuqcyJSMVgATWOjVYQQ7mELCr4A0GReRXn8V0pU7V7CmPNfQBaWVj5nkWJz0pJRvBzjVAlqJ5msxyHCGT4TOdzqISZ3tBEkQFlikmTWpQIoXf70C8U9b1oRRM9D0nd1egiOzon9aOoSMyPPMsCxPf%2FUioHAHbq%2B5wupwDNtnXDTcThGzPgDYxXcLDvRuW6jV4OSNo4oRNdL82cGHN91vFnoc2MSsMsRgAPvqyn%2BOc1D8cOLAl2ry351Iggp0IyhEkJ7qOSoHPkTM%2F%2FNPmicpv6mXIARUWaX%2Fonucs8PmjXSBKhUznmduU1ZMQbrqR0YnedoVwB%2F%2F1aALBsgFvGSOJujY%2BlLCzmdNR0BzUbqB4Hk8rfklFOf%2BfdylVSS3OhnOH2bT7qgvocbmidjZ24IoypsUGxMiLMBEHUH25yKcezuSt1JIQ04VHos7nash6QNoXofnNeF9CYJUoteAKFk7L6fEpe2ZT%2Fqh%2Fvbosh1J9aFugnkQE%2BlCSGUcw5%2ByP1X0vELdkCyM8dn0wnLaRvQY6pgHNwiQX3mR1hPH9nC5hqRa%2FcJ9xAuhGv5R%2FdP2Fcsstsy0NdJs1ixILPNNnOmKMVoopiI5DQ9gBO0DwGLiEsvU0m%2B%2BSE080HeSO7VuIXSoimoLNmkOWB%2BTU4K20B4qdwIk247BMZhmkbJCfZTDGfbEqMljhbAitfy7r6vn75zpoBHnH2lVcsUneW2uyCf0W2%2F%2FsWsdnfmArX8ZbhxChvtgj8baVWYM6&X-Amz-Signature=48b15fe9b04cee648220631f6a6b30a6b6ca3a91c0ed71728c00ffaa8df4386e&X-Amz-SignedHeaders=host&x-id=GetObject)


## 专家模块细节


因为Mixtral 8x7B模型架构与LLaMA相比只有FFN块不同，所以我们只关注这块的细节。


我之前看过一个代码解读，他说MoE层是先计算所有专家的输出，然后在选择每个token对应的专家，其实这样的说法是错误的。因为如果每次都计算所有专家的输出，那就不能体现出MoE模型的优势了，8个模型都计算的话，你的计算量是很大的，那么你的耗时也会增加，起不到提速的效果。所以正确的理解是通过门控层来选择专家，然后只有对应的2个专家会进行前向计算，这样就起到了减少计算量和提速的效果。


首先输入经过attention计算后，经过残差连接、Norm层会输入进行专家层，专家层由门控层和8个专家构成。门控层其实就是一个全连接层，其输出结果再经过softmax函数得到各专家的权重，我们会选择权重排名的top2作为当前token要使用的专家。然后对这两个专家的权重重新进行归一化，得到这两个专家的权重，在前向计算的过程中，top2的专家的输出结果会与其对应的权重进行加权求和，最终我们就能得到整个输入使用不同的专家的结果。


整个流程是比较简单的，可能容易弄混的地方在于模型的输入是**token粒度**的，所以我们在计算权重和选择专家的过程中，都是以每个token为视角的。也就是每一个token都会计算专家权重，并选择top2的专家计算前向结果，最终得到的是整个输入的结果。这个过程结合代码看会更加清晰。


## 专家模块代码注释


下边的代码是专家模块的实现方法，我对重要部分写上了注释，可以结合注释进行理解。


```python
class MixtralSparseMoeBlock(nn.Module):
    """
    This implementation is
    strictly equivalent to standard MoE with full capacity (no
    dropped tokens). It's faster since it formulates MoE operations
    in terms of block-sparse operations to accomodate imbalanced
    assignments of tokens to experts, whereas standard MoE either
    (1) drop tokens at the cost of reduced performance or (2) set
    capacity factor to number of experts and thus waste computation
    and memory on padding.
    """

    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.intermediate_size
        self.num_experts = config.num_local_experts
        self.top_k = config.num_experts_per_tok

        # gating
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)

        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP(config) for _ in range(self.num_experts)])

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """ """
        # 由attention计算后输出的hidden_states作为输入
        batch_size, sequence_length, hidden_dim = hidden_states.shape
        # 将hidden_states构建成一个二维的形状，用于处理每一个token
        hidden_states = hidden_states.view(-1, hidden_dim)
        # router_logits: (batch * sequence_length, n_experts)
        # 通过门控来生成路由，用来决定每一个token由哪些专家处理
        router_logits = self.gate(hidden_states)

        # 通过softmax计算每一个专家对于每个token的处理权重
        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
        # 选取每个token的前top_k个专家和其对应的权重  selected_experts: (batch * sequence_length, top_k)
        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
        # 对每一个token对应的专家的权重值进行归一化，使其权重之和为1
        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)
        # we cast back to the input dtype
        routing_weights = routing_weights.to(hidden_states.dtype)

        # final_hidden_states用来存储每个token对应的专家结果，初始值为0
        final_hidden_states = torch.zeros(
            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device
        )

        # One hot encode the selected experts to create an expert mask
        # this will be used to easily index which expert is going to be sollicitated
        # 使用one hot编码来代表每个token使用哪些专家
        # one hot: (batch * sequence_length, top_k, num_experts) => expert_mask: (num_experts, top_k, batch * sequence_length)
        # 这样做的好处就是，用专家的视角，每次遍历只需要遍历每个专家所需要处理的token即可，否则需要遍历每个token使用了哪个专家，前向的次数随着文本的长度线性增加。
        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)

        # Loop over all available experts in the model and perform the computation on each expert
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            # idx代表当前专家作为top1需要负责的token索引、作为top2需要负责的token的索引
            # top_x代表当前专家负责的token的索引位置。
            idx, top_x = torch.where(expert_mask[expert_idx])

            # 如果top_x中没有1，则代表当前专家不负责任何token，就跳过这个专家
            if top_x.shape[0] == 0:
                continue

            # in torch it is faster to index using lists than torch tensors
            top_x_list = top_x.tolist()
            idx_list = idx.tolist()

            # Index the correct hidden states and compute the expert hidden state for
            # the current expert. We need to make sure to multiply the output hidden
            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
            # 根据索引从输入的隐向量中取得对应的向量，传入到专家模型中进行前向计算
            current_state = hidden_states[None, top_x_list].reshape(-1, hidden_dim)
            current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]

            # However `index_add_` only support torch tensors for indexing so we'll use
            # the `top_x` tensor here.
            # 将当前专家模型的输出写入到预先定义好的final_hidden_states中
            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)
        return final_hidden_states, router_logits
```


> 🚀 如果上边的代码看一遍不好理解的话，可以看一下下边的简化版本，结合debug的输出看更容易理解。


下图中的代码代表了选择专家的简单逻辑，假设当前有10个token和4个专家，每个token选择2个专家。expert_mask输出的结果就是每个专家需要负责的token，我们拆开来看。


在第一次遍历的过程中，我们遍历的是第一个专家，可以看到他是一个(2, 10)的矩阵，第一行代表了当前专家作为top1负责的token，第二行代表了当前专家作为top2负责的token。


我们通过`torch.where(expert_mask)` 来进行解析这个结果，得到的`idx` 中的0代表了当前专家作为top1需要负责的token，1代表了当前专家作为top2需要负责的token，对应的`top_x` 则代表了当前专家负责的token的索引位置，将`idx`和`top_x` 组合就得到了当前专家作为top1、top2负责的token的索引，例如(0,2)、(0,8)、(0,9)、(1,0)、(1,1)、(1,5)，对应的意思就是当前专家作为top1负责的token索引为2、8、9，当前专家作为top2负责的token索引为0、1、5。


![](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/7a072245-b5a2-42a6-92be-0d87ff5bf77c/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466VPS2P5VE%2F20250206%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250206T072247Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjED8aCXVzLXdlc3QtMiJGMEQCIEnVbzYnO%2Fos0uJKmM4ALApdLEDreN9al87BzNk%2BMQuJAiBvTKEKYqk3LCkNiTYjeaQQZThIIzidiqYe5gtIO2YyoSr%2FAwhYEAAaDDYzNzQyMzE4MzgwNSIMnHUqvuO%2FFcFOdvkiKtwDbq0SSihwCXMvkZX4oDal5XpwLLoK%2Fg1DV9tjOYp0zwZFW7UQzaOVZ6hxresZyEenda9rHY5OcUF6E8Wsm5sCPuyrxrecMC4VQAE1mNiV33Sm8cqsuqcyJSMVgATWOjVYQQ7mELCr4A0GReRXn8V0pU7V7CmPNfQBaWVj5nkWJz0pJRvBzjVAlqJ5msxyHCGT4TOdzqISZ3tBEkQFlikmTWpQIoXf70C8U9b1oRRM9D0nd1egiOzon9aOoSMyPPMsCxPf%2FUioHAHbq%2B5wupwDNtnXDTcThGzPgDYxXcLDvRuW6jV4OSNo4oRNdL82cGHN91vFnoc2MSsMsRgAPvqyn%2BOc1D8cOLAl2ry351Iggp0IyhEkJ7qOSoHPkTM%2F%2FNPmicpv6mXIARUWaX%2Fonucs8PmjXSBKhUznmduU1ZMQbrqR0YnedoVwB%2F%2F1aALBsgFvGSOJujY%2BlLCzmdNR0BzUbqB4Hk8rfklFOf%2BfdylVSS3OhnOH2bT7qgvocbmidjZ24IoypsUGxMiLMBEHUH25yKcezuSt1JIQ04VHos7nash6QNoXofnNeF9CYJUoteAKFk7L6fEpe2ZT%2Fqh%2Fvbosh1J9aFugnkQE%2BlCSGUcw5%2ByP1X0vELdkCyM8dn0wnLaRvQY6pgHNwiQX3mR1hPH9nC5hqRa%2FcJ9xAuhGv5R%2FdP2Fcsstsy0NdJs1ixILPNNnOmKMVoopiI5DQ9gBO0DwGLiEsvU0m%2B%2BSE080HeSO7VuIXSoimoLNmkOWB%2BTU4K20B4qdwIk247BMZhmkbJCfZTDGfbEqMljhbAitfy7r6vn75zpoBHnH2lVcsUneW2uyCf0W2%2F%2FsWsdnfmArX8ZbhxChvtgj8baVWYM6&X-Amz-Signature=a932125780923c8b46e3ed79f78933254cfc39fc6fc553ae719a6ac7656e79f0&X-Amz-SignedHeaders=host&x-id=GetObject)


## 更多阅读资料


[Mistral AI官网对Mixtral 8x7B介绍](https://mistral.ai/news/mixtral-of-experts/)


[huggingface 混合专家模型 (MoE) 详解](https://huggingface.co/blog/zh/moe)


[这篇文章提出的关于Mixtral 8x7B的几个问题很有意思，比如训练的时候几个专家同时训练、8个专家的贡献度怎么样](https://zhuanlan.zhihu.com/p/674751021)

